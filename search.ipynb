{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install requirements\n",
        "!wget https://github.com/eur-nl/bongaerts-10k-rag/raw/refs/heads/main/chroma.sqlite3\n",
        "!pip install langchain-community\n",
        "!pip install langchain-chroma\n",
        "!pip install langchain-huggingface\n",
        "!pip install langchain-ollama"
      ],
      "metadata": {
        "id": "nRjeG7luSwYB"
      },
      "id": "nRjeG7luSwYB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b1939a0b",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "b1939a0b"
      },
      "outputs": [],
      "source": [
        "# Import requirements\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "from chromadb.config import Settings\n",
        "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_ollama import ChatOllama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1767c233",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "1767c233"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Pick a segment from the vector database for RAG. Possible values:\n",
        "\"500\" - static chunks with 500 characters\n",
        "\"1000\" - static chunks with 1000 characters\n",
        "\"2500\" - static chunks with 2500 characters\n",
        "\"optimized\" - dynamic optimized chunks\n",
        "\"\"\"\n",
        "db_segment = \"2500\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the database and embeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")"
      ],
      "metadata": {
        "id": "4Hu0qfEzSZ3T"
      },
      "id": "4Hu0qfEzSZ3T",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71e246ad",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "71e246ad"
      },
      "outputs": [],
      "source": [
        "# Initialize the vector dabase\n",
        "db_segments = [\"500\", \"1000\", \"2500\", \"optimized\"]\n",
        "\n",
        "if db_segment not in db_segments:\n",
        "    db_segment = \"optimized\"\n",
        "\n",
        "vector_db = Chroma(\n",
        "    persist_directory=\"/content\",\n",
        "    embedding_function=embeddings,\n",
        "    collection_name=f\"10k_{db_segment}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install and run ollama\n",
        "os.environ.update({'OLLAMA_HOST': '0.0.0.0'})\n",
        "!curl https://ollama.ai/install.sh | sh\n",
        "!nohup ollama serve > ollama.log 2>&1 &\n",
        "!ollama pull llama3"
      ],
      "metadata": {
        "id": "9qVJP-s6pHyD"
      },
      "id": "9qVJP-s6pHyD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "fe85098f",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "fe85098f"
      },
      "outputs": [],
      "source": [
        "llm = ChatOllama(model=\"llama3.2:1b\")\n",
        "\n",
        "QUERY_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
        "    different versions of the given user question to retrieve relevant documents from\n",
        "    a vector database. By generating multiple perspectives on the user question, your\n",
        "    goal is to help the user overcome some of the limitations of the distance-based\n",
        "    similarity search. Provide these alternative questions separated by newlines.\n",
        "    Original question: {question}\"\"\",\n",
        ")\n",
        "\n",
        "retriever = MultiQueryRetriever.from_llm(\n",
        "    vector_db.as_retriever(),\n",
        "    llm,\n",
        "    prompt=QUERY_PROMPT\n",
        ")\n",
        "\n",
        "template = \"\"\"Answer the question based ONLY on the following context:\n",
        "{context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = (\n",
        "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "        | prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask a question\n",
        "query = \"Which products are mentioned?\"\n",
        "print(chain.invoke(query))"
      ],
      "metadata": {
        "id": "O8kk9_l54mVG"
      },
      "id": "O8kk9_l54mVG",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}