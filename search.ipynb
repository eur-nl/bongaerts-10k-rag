{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "F52JdG29KG6p"
      },
      "id": "F52JdG29KG6p"
    },
    {
      "cell_type": "markdown",
      "source": [
        "First install and import all requirements. This will also boot the LLM. The output contains errors relating to ollama, which can safely be ignored."
      ],
      "metadata": {
        "id": "GbzFzdBBb953"
      },
      "id": "GbzFzdBBb953"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install requirements\n",
        "!rm /content/chroma.sqlite3*\n",
        "!wget https://github.com/eur-nl/bongaerts-10k-rag/raw/refs/heads/main/chroma.sqlite3\n",
        "!pip install langchain-community\n",
        "!pip install langchain-chroma\n",
        "!pip install langchain-huggingface\n",
        "!pip install langchain-ollama\n",
        "\n",
        "# Import requirements\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "from chromadb.config import Settings\n",
        "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "# Get the database and embeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "# Install and run ollama\n",
        "!curl https://ollama.ai/install.sh | sh\n",
        "!nohup ollama serve > ollama.log 2>&1 &\n",
        "!ollama pull llama3.2:1b"
      ],
      "metadata": {
        "id": "nRjeG7luSwYB"
      },
      "id": "nRjeG7luSwYB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next pick which segment of the vector database you would like to use for RAG, the possible values are 500, 1000, 2500 and optimized. Only the variable with the very explicit needs to be edited in this cell. This cell will also give some errors related to chromadb which can also be ignored."
      ],
      "metadata": {
        "id": "9TR3NcifgW6R"
      },
      "id": "9TR3NcifgW6R"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71e246ad",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "71e246ad"
      },
      "outputs": [],
      "source": [
        "# Initialize the vector dabase\n",
        "\"\"\"\n",
        "Pick a segment from the vector database for RAG. Possible values:\n",
        "\"500\" - static chunks with 500 characters\n",
        "\"1000\" - static chunks with 1000 characters\n",
        "\"2500\" - static chunks with 2500 characters\n",
        "\"optimized\" - dynamic optimized chunks\n",
        "\"\"\"\n",
        "db_segment = \"optimized\"  # EDIT THIS TO CHANGE SEGMENT\n",
        "\n",
        "if db_segment not in [\"500\", \"1000\", \"2500\", \"optimized\"]:\n",
        "    db_segment = \"optimized\"\n",
        "\n",
        "vector_db = Chroma(\n",
        "    persist_directory=\"/content\",\n",
        "    embedding_function=embeddings,\n",
        "    collection_name=f\"10k_{db_segment}\"\n",
        ")\n",
        "\n",
        "# Prepare the LLM\n",
        "llm = ChatOllama(model=\"llama3.2:1b\", temperature=0.8)\n",
        "\n",
        "QUERY_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"\"\"\n",
        "    You are an AI language model assistant. Your task is to generate five\n",
        "    different versions of the given user question to retrieve relevant documents from\n",
        "    a vector database. By generating multiple perspectives on the user question, your\n",
        "    goal is to help the user overcome some of the limitations of the distance-based\n",
        "    similarity search. Provide these alternative questions separated by newlines.\n",
        "    Original question: {question}\"\"\",\n",
        ")\n",
        "\n",
        "retriever = MultiQueryRetriever.from_llm(\n",
        "    vector_db.as_retriever(),\n",
        "    llm,\n",
        "    prompt=QUERY_PROMPT\n",
        ")\n",
        "\n",
        "template = \"\"\"You are an assistant for question-answering tasks for Retrieval Augmented Generation system for the 10K financial reports.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "Keep the answer concise.\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = (\n",
        "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "        | prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally ask your question here. This may take a while. The chromadb error in this cell can also be ignored."
      ],
      "metadata": {
        "id": "GdYPyRlrgvd5"
      },
      "id": "GdYPyRlrgvd5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask a question\n",
        "query = \"Who is the CEO of Apple?\"\n",
        "print(chain.invoke(query))"
      ],
      "metadata": {
        "id": "O8kk9_l54mVG"
      },
      "id": "O8kk9_l54mVG",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}